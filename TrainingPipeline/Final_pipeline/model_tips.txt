
Batch sizes:
16

Learning rate: 
3e-4, 1e-4, 5e-5, 3e-5

Swedish pretrained models to test:

1.) KB/bert-base-swedish-cased
2.) AI-Nordics/bert-large-swedish-cased
3.) For fun : xlm-roberta-large 

# lr_scheduler_type:
# LINEAR="linear"
# COSINE = "cosine"
# COSINE_WITH_RESTARTS = "cosine_with_restarts"
# POLYNOMIAL = "polynomial"
# CONSTANT = "constant"
# CONSTANT_WITH_WARMUP = "constant_with_warmup"
# warmup_steps=50


# SAVED CODE

# Splitting data into train,test,valid

# train_testvalid = dataset.train_test_split(test_size=0.1)
# # Split the 10% test + valid in half test, half valid
# test_valid = train_testvalid['test'].train_test_split(test_size=0.1)
# # gather everyone if you want to have a single DatasetDict
# train_test_valid_dataset = DatasetDict({
#     'train': train_testvalid['train'],
#     'test': test_valid['test'],
#     'valid': test_valid['train']})
# full_datasets = train_test_valid_dataset


from sklearn.model_selection import train_test_split

# Testing tokenization
print(test[0])
token = tokenized_test[0]['input_ids']
print(token)
decoded = tokenizer.decode(tokenized_test[0]['input_ids'])
print(decoded)


# Saving and splitting new dataset
train_test_split = 0.3
test_valid_split = 0.5
# full_datasets = load_split_data(finetune_dataset, train_test_split, test_valid_split)
# full_datasets.save_to_disk("./Local/Test/")


# Split into test train only
# dataset = load_dataset('json', data_files= jsonl_file)['train']
# dataset = dataset.remove_columns(["id","thread_id","thread","keyword","starting_index","span_length"])
# dataset = dataset.rename_column("TOXIC", "label")
#
# train_test = dataset.train_test_split(test_size=train_test_split)
# train_test_dataset = DatasetDict({
#     'train': train_test['train'],
#     'test': train_test['test']
# })


# compute custom loss (suppose one has 2 labels with different weights)
#loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 1.0])) # Can weigh the labels differently? , This lines gives gpu bug , tensor not set to correct device


# tokenize_data()
    # Creating subsets
    # small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(10)) # 1000
    # small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(10))   # 1000
    # full_train_dataset = tokenized_datasets["train"]
    # full_valid_dataset = tokenized_datasets["valid"]
    # full_test_dataset = tokenized_datasets["test"]

    #tokenized_datasets = []
    # tokenized_datasets.append(small_train_dataset)
    # tokenized_datasets.append(small_eval_dataset)
    # tokenized_datasets.append(full_train_dataset)
    # tokenized_datasets.append(full_valid_dataset)
    # tokenized_datasets.append(full_test_dataset)